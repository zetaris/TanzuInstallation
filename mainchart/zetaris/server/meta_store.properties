jdbc_url=${METASTORE_JDBC_URL}
jdbc_driver_class=${METASTORE_JDBC_DRIVER_CLASS}
username=${METASTORE_USERNAME}
password=${METASTORE_PASSWORD}
hibernate.dialect=${METASTORE_HIBERNATE_DIALECT}
hibernate.default_schema=
hibernate.show_sql=false
#hive meta store
hive_meta_jdbc_url=${HIVE_META_JDBC_URL}
hive_meta_jdbc_driver_class=${HIVE_META_JDBC_DRIVER_CLASS}
hive_meta_username=${HIVE_META_USERNAME}
hive_meta_password=${HIVE_META_PASSWORD}
hive.metastore.schema.verification=false
org.jpox.autoCreateSchema=true
datanucleus.autoStartMechanism=true
org.jpox.fixedDatastore=true
#audit meta store
auditLog_jdbc_url=${AUDIT_LOG_JDBC_URL}
auditLog_jdbc_driver_class=${AUDIT_LOG_JDBC_DRIVER_CLASS}
auditLog_username=${AUDIT_LOG_USERNAME}
auditLog_password=${AUDIT_LOG_PASSWORD}
auditLog_hibernate.dialect=${AUDIT_LOG_HIBERNATE_DIALECT}
auditLog_hibernate.default_schema=
auditLog_hibernate.show_sql=false
auditLog.enable=true

#turn on/off audit log
auditLog.enable=true

# connection pool setting for meta store
poolsize.initialSize=1
poolsize.maxTotal=50

# default admin credentials                                                                                                                                                                  
default_admin_email=admin@default.com
default_admin_password=

#landing zone for data ingestion
landing_zone=/Users/jaesungjun/zetaris_data_hub/landing
staging_zone=/Users/jaesungjun/zetaris_data_hub/staging
manual_zone=/Users/jaesungjun/zetaris_data_hub/staging/manual
landing_zone_watch_interval_in_sec=3

#zeppelin notebook host port
notebook_host=localhost
notebook_port=8080
notebook_dir_for_data_quality_management=/wib-dq
notebook_dir_for_advanced_analytics=/zetaris-advanced-analytics

#Running Mode(STANDALONE | SAAS_MAIN | SAAS_SQL_WORKBENCH | SAAS_STREAMING)
running_mode=STANDALONE

#kafka bootstrap servers, this is used in both STANDALONE AND SAAS_MAIN mode
#kafka_bootstrap_servers=kafka1:9092,kafka2:9092
kafka_bootstrap_servers=localhost:9092

#kafka user credential, these are used for push data source
kafka_accesskey=$ConnectionString
kafka_securitykey=change_me

#waiting in millis to get topic meta data after creating topic
kafka_topic_creation_wait_in_millis=5000
f
#waiting in millis to start reading message in topic. Topic will be closed automatically if there is no message in this waiting time
kafka_topic_read_wait_in_millis=30000

#publish thread count
kafka_query_publisher_thread_count=10

#Lightning REST service port
jdbc_endpoint_port=9998

#data warehouse api host
warehouse_provisioning_server_host=localhost
warehouse_provisioning_server_port=9010

#data warehouse monitor interval in second
data_warehouse_monitor_interval=10

# The combination of lightning_cloud_rep_id and kafka_push_data_source_topic is used for push type data source
#representative(organisation) id in Lightning cloud
lightning_cloud_rep_id=

#kafka topic for push data source. This is provided by cloud saas
kafka_push_data_source_topic=push_ds

#kafka topic partition id. this is provided by cloud saas
kafka_push_data_source_partition_id=0

#maximum time to block in millis to poll the topic
poll_topic_to_block_in_millis=5000

#maximum retry to poll the topic
poll_topic_retry_threshold=3

#bound organisation id for spin off d/w cluster
#Provisioning server will change when spinning off d/w cluster in case of SAAS_SQL_WORKBENCH mode
#Don't change this explicitly
bound_organisation_id=default

#bound d/w name for spin off d/w cluster
bound_data_warehouse_name=change_me

#threshold d/w idle time, SAAS will auto shutdown when the idle is lasting form than this threshold
#negative turn off this
auto__shutdown_data_warehouse_idle_in_min=5

#local file system root
filesystem_staging = "data"
filesystem_staging = ${?ZFILESYSTEM_STAGING}

#authentication
enable_account_lock=true
login_attempts_limit=3
login_duration_in_minutes=10

#authorization_provider, LIGHTNING or OPA or FILE
authorization_provider=FILE

#In case of OPA authorization, OPA server and port
opa_server=localhost
opa_port=8181
opa_url_prefix=/v1/data/lpoc_7/schema

#In case of FILE, root directory. orgid > data source > table.json
file_auth_provider_root=${LIGHTNING_HOME}/acl

# solr server instance
solr_server=lightning-solr-svc:8983/solr/

enable_adaptive_cache=true

# ignite, hdfs, spark, nil
lightning_cache_provider=hdfs,spark
# configuration file for offsite cache
lightning_cache_config=${LIGHTNING_HOME}/conf/config-client.xml

# hdfs cache setting, file format - delta or parquet
lightning_filecache_format=delta


# hdfs cache setting, file format - root path
lightning_filecache_root_path=
lightning_filecache_azure_secret_key=
lightning_filecache_azure_secret_val=
lightning_filecache_s3_access_key=
lightning_filecache_s3_secret_key=

# cpu core to scan data source if the table has number type column with index. use all available cpu core if -1
datasource_scan_cpu_core=5

ndp_driver_truststore=${LIGHTNING_HOME}/data/key-trust-store/ndp_driver_truststore.p12
ndp_driver_truststore_passwd=change_it

#hive metastore connection time out
hivemetastore_timeout_in_sec=2

#default shuffle partition for streaming aggregation
stream_aggr_shuffle_partitions=2

db_fields_encryption_private_key=${LIGHTNING_HOME}/password-security/private_key.der
db_fields_encryption_public_key=${LIGHTNING_HOME}/password-security/public_key.der

#aws glue metastores configuration file
aws_glue_config_path=${LIGHTNING_HOME}/conf/aws_glue_conf.properties
aws_glue_special_chars_regex=\\W

#ignore deleted partition for hive/glue table
ignore_deleted_parts=true

#set false if a separate cache/sync server up & running.
#cache/sync server is doing metadata surveilance & syncing cache
cache_sync_enable=true

#turn on/off heterogenous query optimizer
#turn on/off predicate in native expression
hqo_predicate_enable=true

#turn on/off limit
hgo_limit_enable=true

#turn on/off  aggregation
hgo_aggregation_enable=true

#turn on/off convert type
hgo_convert_type_enable=true


is_rest_service_enabled=true
base_endpoint=http://lightning-api-svc
refresh_period_in_minutes=10

